{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2j-zA0if54o",
    "outputId": "f365232e-231f-4ebe-c2a7-e9e7a48ce96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 任务1: C=10, gamma=1 ---\n",
      "  模拟 1/13: 训练误差=0.0000, 测试误差=0.1250, SV数=144, 正确离群点数=0\n",
      "  模拟 2/13: 训练误差=0.0000, 测试误差=0.0625, SV数=144, 正确离群点数=0\n",
      "  模拟 3/13: 训练误差=0.0000, 测试误差=0.1875, SV数=143, 正确离群点数=0\n",
      "  模拟 4/13: 训练误差=0.0000, 测试误差=0.1250, SV数=141, 正确离群点数=0\n",
      "  模拟 5/13: 训练误差=0.0000, 测试误差=0.1250, SV数=146, 正确离群点数=0\n",
      "  模拟 6/13: 训练误差=0.0000, 测试误差=0.0625, SV数=145, 正确离群点数=0\n",
      "  模拟 7/13: 训练误差=0.0000, 测试误差=0.1875, SV数=145, 正确离群点数=0\n",
      "  模拟 8/13: 训练误差=0.0000, 测试误差=0.3125, SV数=144, 正确离群点数=0\n",
      "  模拟 9/13: 训练误差=0.0000, 测试误差=0.1250, SV数=141, 正确离群点数=0\n",
      "  模拟 10/13: 训练误差=0.0000, 测试误差=0.0625, SV数=140, 正确离群点数=0\n",
      "  模拟 11/13: 训练误差=0.0000, 测试误差=0.1250, SV数=145, 正确离群点数=0\n",
      "  模拟 12/13: 训练误差=0.0000, 测试误差=0.0000, SV数=139, 正确离群点数=0\n",
      "  模拟 13/13: 训练误差=0.0000, 测试误差=0.0625, SV数=140, 正确离群点数=0\n",
      "\n",
      "任务1 (C=10.0, gamma=1.0) 结果:\n",
      "  平均训练误差: 0.0000 (标准差: 0.0000)\n",
      "  平均测试误差: 0.1202 (标准差: 0.0754)\n",
      "  平均支持向量数量: 142.85\n",
      "  平均正确分类的离群点数量: 0.00\n",
      "------------------------------\n",
      "\n",
      "--- 任务2: 不同C值下的分析 (gamma=1) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_slsqp_py.py:435: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  fx = wrapped_fun(x)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_slsqp_py.py:439: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  g = append(wrapped_grad(x), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C=0.01: Avg SVs=184.08, Avg Correct Outliers=84.31, Avg Incorrect Outliers=89.54\n",
      "  C=0.1: Avg SVs=183.38, Avg Correct Outliers=89.92, Avg Incorrect Outliers=84.31\n",
      "  C=1: Avg SVs=153.77, Avg Correct Outliers=62.54, Avg Incorrect Outliers=1.77\n",
      "  C=10: Avg SVs=142.85, Avg Correct Outliers=0.00, Avg Incorrect Outliers=0.00\n",
      "  C=50: Avg SVs=142.85, Avg Correct Outliers=0.00, Avg Incorrect Outliers=0.00\n",
      "  C=100: Avg SVs=142.85, Avg Correct Outliers=0.00, Avg Incorrect Outliers=0.00\n",
      "\n",
      "任务2 表格结果[cite: 10]:\n",
      "        C Average number of support vectors Average number of correct outliers Average number of incorrect outliers\n",
      "0    0.01                            184.08                              84.31                                89.54\n",
      "1    0.10                            183.38                              89.92                                84.31\n",
      "2    1.00                            153.77                              62.54                                 1.77\n",
      "3   10.00                            142.85                               0.00                                 0.00\n",
      "4   50.00                            142.85                               0.00                                 0.00\n",
      "5  100.00                            142.85                               0.00                                 0.00\n",
      "\n",
      "任务2 趋势分析:\n",
      "1. C 与支持向量数量的关系:\n",
      "   - 当 C 值较小时，模型对误分类的惩罚较小，倾向于选择一个更宽的间隔（margin），即使这意味着一些点会落入间隔内或被错误分类。这通常导致较少的支持向量，主要是那些定义间隔边界的点。\n",
      "   - 当 C 值增大时，模型对误分类的惩罚变大。模型会试图将尽可能多的点正确分类，即使这意味着间隔会变窄。这会导致更多的点成为支持向量，因为模型对数据拟合得更紧密。\n",
      "   - 因此，通常情况下，随着 C 的增加，支持向量的数量会增加或保持在一个较高的水平，因为更多的点会影响决策边界的确定。\n",
      "\n",
      "2. C 与正确/错误分类的离群点数量的关系 (离群点指 alpha_i = C 的点):\n",
      "   - 正确分类的离群点 (alpha_i=C, y_i * f(x_i) > 0): 这些是位于间隔边界上或间隔内但被正确分类的点。\n",
      "   - 错误分类的离群点 (alpha_i=C, y_i * f(x_i) < 0): 这些是被错误分类的点。\n",
      "   - 当 C 值较小时，模型容忍更多的错误和间隔内的点。可能有较多的点其 alpha 达到 C，其中一些可能是被容忍的错误分类（错误离群点），另一些可能是间隔内被正确分类的点（正确离群点）。\n",
      "   - 当 C 值增大时，模型对错误的容忍度降低。\n",
      "     - 错误分类的离群点数量可能会减少，因为模型努力避免它们。然而，如果数据非常复杂或有噪声，即使C很大，也可能存在一些顽固的错误分类点。\n",
      "     - 正确分类的离群点（那些在间隔内但被正确分类的点，alpha=C）的数量可能会变化。如果C很大，模型试图使间隔“干净”，可能会减少这类点。但同时，更多的点可能成为边界上的支持向量（alpha < C）。对于alpha=C的点，它们是那些模型“尽力”也无法使其满足 y_i*f(x_i) >= 1 的点。\n",
      "   - 总的来说，随着 C 增加：\n",
      "     - 模型对训练数据的拟合程度增加。\n",
      "     - 错误分类的离群点数量（alpha=C且错分）一般期望减少，因为惩罚变大了。\n",
      "     - 正确分类的离群点数量（alpha=C且正分，但 $y_i f(x_i) \\le 1$）的行为可能更复杂，取决于数据集。如果C非常大，模型会尝试将所有点都正确分类并推出间隔，所以理论上这类点（alpha=C）应该主要是那些难以处理的点。\n",
      "------------------------------\n",
      "\n",
      "--- 任务3: 超参数调优 ---\n",
      "开始超参数调优，Gamma候选: [0.01, 0.1, 0.5, 1, 5, 10], C候选: [0.1, 1, 10, 50, 100, 200]\n",
      "  Gamma=0.01, C=0.1: 平均测试误差 = 0.4663\n",
      "  Gamma=0.01, C=1: 平均测试误差 = 0.4375\n",
      "  Gamma=0.01, C=10: 平均测试误差 = 0.2212\n",
      "  Gamma=0.01, C=50: 平均测试误差 = 0.1827\n",
      "  Gamma=0.01, C=100: 平均测试误差 = 0.1827\n",
      "  Gamma=0.01, C=200: 平均测试误差 = 0.1875\n",
      "  Gamma=0.1, C=0.1: 平均测试误差 = 0.4663\n",
      "  Gamma=0.1, C=1: 平均测试误差 = 0.2019\n",
      "  Gamma=0.1, C=10: 平均测试误差 = 0.1394\n",
      "  Gamma=0.1, C=50: 平均测试误差 = 0.1250\n",
      "  Gamma=0.1, C=100: 平均测试误差 = 0.1346\n",
      "  Gamma=0.1, C=200: 平均测试误差 = 0.1538\n",
      "  Gamma=0.5, C=0.1: 平均测试误差 = 0.3750\n",
      "  Gamma=0.5, C=1: 平均测试误差 = 0.1442\n",
      "  Gamma=0.5, C=10: 平均测试误差 = 0.0962\n",
      "  Gamma=0.5, C=50: 平均测试误差 = 0.0962\n",
      "  Gamma=0.5, C=100: 平均测试误差 = 0.0962\n",
      "  Gamma=0.5, C=200: 平均测试误差 = 0.0962\n",
      "  Gamma=1, C=0.1: 平均测试误差 = 0.4519\n",
      "  Gamma=1, C=1: 平均测试误差 = 0.1346\n",
      "  Gamma=1, C=10: 平均测试误差 = 0.1202\n",
      "  Gamma=1, C=50: 平均测试误差 = 0.1202\n",
      "  Gamma=1, C=100: 平均测试误差 = 0.1202\n",
      "  Gamma=1, C=200: 平均测试误差 = 0.1202\n",
      "  Gamma=5, C=0.1: 平均测试误差 = 0.4663\n",
      "  Gamma=5, C=1: 平均测试误差 = 0.1731\n",
      "  Gamma=5, C=10: 平均测试误差 = 0.1683\n",
      "  Gamma=5, C=50: 平均测试误差 = 0.1683\n",
      "  Gamma=5, C=100: 平均测试误差 = 0.1683\n",
      "  Gamma=5, C=200: 平均测试误差 = 0.1683\n",
      "  Gamma=10, C=0.1: 平均测试误差 = 0.4663\n",
      "  Gamma=10, C=1: 平均测试误差 = 0.3077\n",
      "  Gamma=10, C=10: 平均测试误差 = 0.2981\n",
      "  Gamma=10, C=50: 平均测试误差 = 0.2981\n",
      "  Gamma=10, C=100: 平均测试误差 = 0.2981\n",
      "  Gamma=10, C=200: 平均测试误差 = 0.2981\n",
      "\n",
      "任务3 超参数调优结果:\n",
      "  最佳平均测试误差: 0.0962\n",
      "  对应的最佳 Gamma: 0.5\n",
      "  对应的最佳 C: 10\n",
      "------------------------------\n",
      "\n",
      "代码执行完毕。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "\n",
    "# --- 数据加载和预处理 ---\n",
    "def load_sonar_data(file_path):\n",
    "    \"\"\"\n",
    "    加载声纳数据集。\n",
    "    将标签 'M' (雷) 转换为 1, 'R' (岩石) 转换为 -1。\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y_str = data.iloc[:, -1].values\n",
    "    y = np.array([1 if label == 'M' else -1 for label in y_str])\n",
    "    return X, y\n",
    "\n",
    "# --- RBF 核函数 ---\n",
    "def rbf_kernel(x1, x2, gamma):\n",
    "    \"\"\"\n",
    "    计算两个样本点之间的RBF核值。\n",
    "    K(x1, x2) = exp(-gamma * ||x1 - x2||^2)\n",
    "    \"\"\"\n",
    "    return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)\n",
    "\n",
    "# --- SVM 训练函数 ---\n",
    "def train_svm(X_train, y_train, C, gamma, tol=1e-5):\n",
    "    \"\"\"\n",
    "    使用SLSQP算法训练SVM。\n",
    "\n",
    "    参数:\n",
    "    X_train (np.array): 训练特征数据 (n_samples, n_features)\n",
    "    y_train (np.array): 训练标签数据 (n_samples,)\n",
    "    C (float): 惩罚参数\n",
    "    gamma (float): RBF核的参数\n",
    "    tol (float): 用于比较浮点数的容差\n",
    "\n",
    "    返回:\n",
    "    alpha_sv (np.array): 支持向量的alpha值\n",
    "    X_sv (np.array): 支持向量的特征\n",
    "    y_sv (np.array): 支持向量的标签\n",
    "    b (float): 偏置项\n",
    "    all_alphas (np.array): 所有训练样本的alpha值\n",
    "    \"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "\n",
    "    # 预计算 Gram 矩阵 K_ij = K(x_i, x_j)\n",
    "    K_matrix = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K_matrix[i, j] = rbf_kernel(X_train[i], X_train[j], gamma)\n",
    "\n",
    "    # 对偶问题的目标函数 (最小化)\n",
    "    # L_D = 0.5 * sum_i sum_j alpha_i alpha_j y_i y_j K(x_i, x_j) - sum_i alpha_i\n",
    "    def objective_func(alpha):\n",
    "        # y_i y_j K(x_i, x_j) 部分可以写成 (alpha * y).T @ K_matrix @ (alpha * y)\n",
    "        # 但更直接的方式是使用原始的 alpha 和 K_matrix, 并确保 y_i y_j 在其中\n",
    "        # term1 = 0.5 * np.sum(np.outer(alpha * y_train, alpha * y_train) * K_matrix) # 这种写法可能不完全对，因为 y_i y_j\n",
    "        # 我们需要 alpha_i alpha_j y_i y_j K_ij\n",
    "        # (alpha_i y_i) (alpha_j y_j) K_ij\n",
    "        # (y_train[:, None] * K_matrix * y_train[None, :]) creates Q_ij = y_i y_j K_ij\n",
    "        Q_ij = np.outer(y_train, y_train) * K_matrix\n",
    "        term1 = 0.5 * alpha @ Q_ij @ alpha\n",
    "        term2 = np.sum(alpha)\n",
    "        return term1 - term2\n",
    "\n",
    "    # 目标函数的雅可比矩阵 (梯度)\n",
    "    # dL_D / d_alpha_k = sum_j alpha_j y_k y_j K(x_k, x_j) - 1\n",
    "    #                 = y_k * sum_j alpha_j y_j K(x_k, x_j) - 1\n",
    "    def objective_jac(alpha):\n",
    "        Q_ij = np.outer(y_train, y_train) * K_matrix\n",
    "        grad = Q_ij @ alpha - np.ones(n_samples)\n",
    "        return grad\n",
    "\n",
    "\n",
    "    # 约束条件\n",
    "    # 1. sum(alpha_i * y_i) = 0\n",
    "    constraints = ({'type': 'eq',\n",
    "                      'fun': lambda alpha: np.dot(alpha, y_train),\n",
    "                      'jac': lambda alpha: y_train})\n",
    "\n",
    "    # 边界条件: 0 <= alpha_i <= C\n",
    "    bounds = [(0, C) for _ in range(n_samples)]\n",
    "\n",
    "    # 初始猜测\n",
    "    alpha_init = np.zeros(n_samples)\n",
    "    # alpha_init = np.random.rand(n_samples) * C * 0.1 # 另一种初始猜测\n",
    "\n",
    "    # 使用SLSQP求解\n",
    "    result = minimize(objective_func, alpha_init, jac=objective_jac,\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints,\n",
    "                      tol=1e-6, options={'maxiter': 1000, 'disp': False})\n",
    "\n",
    "    if not result.success:\n",
    "        print(f\"警告: SLSQP优化未成功收敛: {result.message}\")\n",
    "\n",
    "\n",
    "    all_alphas = result.x\n",
    "\n",
    "    # 找出支持向量 (alpha > tol)\n",
    "    sv_indices = np.where(all_alphas > tol)[0]\n",
    "    alpha_sv = all_alphas[sv_indices]\n",
    "    X_sv = X_train[sv_indices]\n",
    "    y_sv = y_train[sv_indices]\n",
    "\n",
    "    if len(alpha_sv) == 0:\n",
    "        # print(\"警告: 没有找到支持向量。可能C值太小或数据特性导致。\")\n",
    "        # 在这种情况下，b和预测可能无意义或需要特殊处理。\n",
    "        # 为了代码能继续运行，我们设定一些默认值，但这表示模型训练可能有问题。\n",
    "        return np.array([]), np.array([]), np.array([]), 0.0, all_alphas\n",
    "\n",
    "\n",
    "    # 计算偏置项 b\n",
    "    # b = y_k - sum_{i in SV} alpha_i y_i K(x_i, x_k)\n",
    "    # 选择 0 < alpha_k < C - tol 的支持向量来计算 b，并取平均值\n",
    "    margin_sv_indices = np.where((all_alphas > tol) & (all_alphas < C - tol))[0]\n",
    "    b_values = []\n",
    "\n",
    "    if len(margin_sv_indices) > 0:\n",
    "        for k_idx in margin_sv_indices:\n",
    "            # k_idx 是在 all_alphas 中的索引\n",
    "            # 我们需要用 X_train[k_idx] 和 y_train[k_idx]\n",
    "            # sum项是对所有支持向量 (alpha_sv, X_sv, y_sv) 计算的\n",
    "            decision_val_k_no_b = 0\n",
    "            for i in range(len(alpha_sv)):\n",
    "                decision_val_k_no_b += alpha_sv[i] * y_sv[i] * rbf_kernel(X_sv[i], X_train[k_idx], gamma)\n",
    "            b_values.append(y_train[k_idx] - decision_val_k_no_b)\n",
    "        b = np.mean(b_values) if b_values else 0\n",
    "    else:\n",
    "        # 如果没有在边界内的支持向量 (0 < alpha < C)，所有SV的alpha都近似为C\n",
    "        # 此时，我们需要更鲁棒的 b 计算方法\n",
    "        # b_upper for y_k = 1, alpha_k = C => f(x_k) + b <= 1 => b <= 1 - f(x_k)\n",
    "        # b_lower for y_k = -1, alpha_k = C => f(x_k) + b >= -1 => b >= -1 - f(x_k)\n",
    "        b_upper_candidates = []\n",
    "        b_lower_candidates = []\n",
    "\n",
    "        for k_idx in sv_indices: # 遍历所有 alpha_k approx C 的支持向量\n",
    "            decision_val_k_no_b = 0\n",
    "            for i in range(len(alpha_sv)): # sum over identified SVs\n",
    "                decision_val_k_no_b += alpha_sv[i] * y_sv[i] * rbf_kernel(X_sv[i], X_train[k_idx], gamma)\n",
    "\n",
    "            if y_train[k_idx] == 1:\n",
    "                b_upper_candidates.append(1 - decision_val_k_no_b)\n",
    "            else: # y_train[k_idx] == -1\n",
    "                b_lower_candidates.append(-1 - decision_val_k_no_b)\n",
    "\n",
    "        b_max_lower = -np.inf\n",
    "        if b_lower_candidates:\n",
    "            b_max_lower = np.max(b_lower_candidates)\n",
    "\n",
    "        b_min_upper = np.inf\n",
    "        if b_upper_candidates:\n",
    "            b_min_upper = np.min(b_upper_candidates)\n",
    "\n",
    "        if b_max_lower > b_min_upper + tol : # 检查是否有矛盾\n",
    "             # print(f\"警告: b_max_lower ({b_max_lower:.4f}) > b_min_upper ({b_min_upper:.4f}). 可能存在数值问题或模型配置问题。\")\n",
    "             # 这种情况下，通常取中点，或者根据具体情况调整。\n",
    "             # 为了简单，这里取中点，但实际应用中需要注意。\n",
    "             # 或者，如果所有 SV 都是同一类，则 b 的范围可能是一侧开放的。\n",
    "             pass # 允许继续，但这是一个警告信号\n",
    "\n",
    "        if b_max_lower != -np.inf and b_min_upper != np.inf:\n",
    "            b = (b_max_lower + b_min_upper) / 2.0\n",
    "        elif b_max_lower != -np.inf:\n",
    "            b = b_max_lower\n",
    "        elif b_min_upper != np.inf:\n",
    "            b = b_min_upper\n",
    "        else: # 如果都没有（例如只有一个SV），就用那个SV计算\n",
    "            # print(\"警告: 计算b时，b_lower 和 b_upper 均无有效值。尝试用第一个SV计算b。\")\n",
    "            if len(sv_indices)>0:\n",
    "                k_idx_first_sv = sv_indices[0]\n",
    "                decision_val_first_sv_no_b = 0\n",
    "                for i in range(len(alpha_sv)):\n",
    "                    decision_val_first_sv_no_b += alpha_sv[i] * y_sv[i] * rbf_kernel(X_sv[i], X_train[k_idx_first_sv], gamma)\n",
    "                b = y_train[k_idx_first_sv] - decision_val_first_sv_no_b\n",
    "            else:\n",
    "                b = 0 # Fallback, should not happen if SVs exist\n",
    "\n",
    "    return alpha_sv, X_sv, y_sv, b, all_alphas\n",
    "\n",
    "# --- SVM 预测函数 ---\n",
    "def predict_svm(X_eval, alpha_sv, X_sv, y_sv, b, gamma):\n",
    "    \"\"\"\n",
    "    使用训练好的SVM模型进行预测。\n",
    "    \"\"\"\n",
    "    if len(alpha_sv) == 0 : # 没有支持向量\n",
    "        # print(\"预测警告：模型没有支持向量，所有预测将基于b值（若b为0，则预测为0）。\")\n",
    "        scores = np.full(X_eval.shape[0], b)\n",
    "        predictions = np.sign(scores)\n",
    "        # 处理 np.sign(0) 的情况，可以将其归为一类，例如 +1\n",
    "        predictions[predictions == 0] = 1\n",
    "        return predictions, scores\n",
    "\n",
    "    scores = np.zeros(X_eval.shape[0])\n",
    "    for i in range(X_eval.shape[0]):\n",
    "        s = 0\n",
    "        for j in range(len(alpha_sv)):\n",
    "            s += alpha_sv[j] * y_sv[j] * rbf_kernel(X_sv[j], X_eval[i], gamma)\n",
    "        scores[i] = s + b\n",
    "    predictions = np.sign(scores)\n",
    "    # 处理 np.sign(0) 的情况，可以将其归为一类，例如 +1\n",
    "    predictions[predictions == 0] = 1\n",
    "    return predictions, scores\n",
    "\n",
    "\n",
    "# --- 主程序 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据 [cite: 2]\n",
    "    file_path = 'sonar.all-data'\n",
    "    X, y = load_sonar_data(file_path)\n",
    "    n_total_samples = X.shape[0] # 208\n",
    "\n",
    "    epsilon_alpha_is_C = 1e-4 # 用于判断 alpha_i 是否等于 C 的容差\n",
    "\n",
    "    # --- 任务1: C=10, gamma=1 的13次模拟 --- [cite: 4, 5, 6, 7, 8]\n",
    "    print(\"--- 任务1: C=10, gamma=1 ---\")\n",
    "    C_val_task1 = 10.0\n",
    "    gamma_val_task1 = 1.0\n",
    "    n_simulations = 13\n",
    "\n",
    "    train_errors_task1 = []\n",
    "    test_errors_task1 = []\n",
    "    num_svs_task1 = []\n",
    "    # “正确分类的离群点”是指那些 alpha_i = C 并且被正确分类的训练样本\n",
    "    correctly_classified_outliers_task1 = [] # [cite: 8]\n",
    "\n",
    "    for sim_idx in range(n_simulations):\n",
    "        # 定义测试集索引 [cite: 7]\n",
    "        test_indices = np.arange(sim_idx, n_total_samples, n_simulations)\n",
    "        train_indices = np.array([i for i in range(n_total_samples) if i not in test_indices])\n",
    "\n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_test, y_test = X[test_indices], y[test_indices]\n",
    "\n",
    "        # 训练SVM\n",
    "        alpha_sv, X_sv_runtime, y_sv_runtime, b_runtime, all_train_alphas = train_svm(X_train, y_train, C_val_task1, gamma_val_task1, tol=1e-5)\n",
    "\n",
    "        if len(alpha_sv) == 0: # 如果没有SV，则跳过此轮模拟的统计\n",
    "            print(f\"  模拟 {sim_idx+1}/{n_simulations}: 未找到支持向量，跳过。\")\n",
    "            # 可以选择记录为失败或使用默认错误率（如0.5或1.0）\n",
    "            train_errors_task1.append(0.5) # 假设错误率\n",
    "            test_errors_task1.append(0.5)  # 假设错误率\n",
    "            num_svs_task1.append(0)\n",
    "            correctly_classified_outliers_task1.append(0)\n",
    "            continue\n",
    "\n",
    "        # 训练集错误率\n",
    "        train_preds, train_scores = predict_svm(X_train, alpha_sv, X_sv_runtime, y_sv_runtime, b_runtime, gamma_val_task1)\n",
    "        train_error = np.mean(train_preds != y_train)\n",
    "        train_errors_task1.append(train_error)\n",
    "\n",
    "        # 测试集错误率\n",
    "        test_preds, _ = predict_svm(X_test, alpha_sv, X_sv_runtime, y_sv_runtime, b_runtime, gamma_val_task1)\n",
    "        test_error = np.mean(test_preds != y_test)\n",
    "        test_errors_task1.append(test_error)\n",
    "\n",
    "        # 支持向量数量\n",
    "        num_svs_task1.append(len(alpha_sv))\n",
    "\n",
    "        # 正确分类的离群点数量\n",
    "        # 离群点是训练集中 alpha_i 约等于 C 的点\n",
    "        # all_train_alphas 是对应 X_train 的所有 alpha 值\n",
    "        outlier_candidate_indices = np.where(np.abs(all_train_alphas - C_val_task1) < epsilon_alpha_is_C)[0]\n",
    "        count_correct_outliers = 0\n",
    "        if len(outlier_candidate_indices) > 0:\n",
    "            # 我们需要对这些离群点候选者进行分类判断\n",
    "            # train_preds 是对整个训练集的预测，可以直接使用\n",
    "            for idx_in_train in outlier_candidate_indices:\n",
    "                # 检查该点是否被正确分类\n",
    "                if train_preds[idx_in_train] == y_train[idx_in_train]:\n",
    "                    count_correct_outliers += 1\n",
    "        correctly_classified_outliers_task1.append(count_correct_outliers)\n",
    "        print(f\"  模拟 {sim_idx+1}/{n_simulations}: 训练误差={train_error:.4f}, 测试误差={test_error:.4f}, SV数={len(alpha_sv)}, 正确离群点数={count_correct_outliers}\")\n",
    "\n",
    "\n",
    "    # 计算均值和标准差 [cite: 6, 8]\n",
    "    avg_train_error_task1 = np.mean(train_errors_task1)\n",
    "    std_train_error_task1 = np.std(train_errors_task1)\n",
    "    avg_test_error_task1 = np.mean(test_errors_task1)\n",
    "    std_test_error_task1 = np.std(test_errors_task1)\n",
    "    avg_num_svs_task1 = np.mean(num_svs_task1)\n",
    "    avg_correct_outliers_task1 = np.mean(correctly_classified_outliers_task1)\n",
    "\n",
    "    print(f\"\\n任务1 (C={C_val_task1}, gamma={gamma_val_task1}) 结果:\")\n",
    "    print(f\"  平均训练误差: {avg_train_error_task1:.4f} (标准差: {std_train_error_task1:.4f})\")\n",
    "    print(f\"  平均测试误差: {avg_test_error_task1:.4f} (标准差: {std_test_error_task1:.4f})\")\n",
    "    print(f\"  平均支持向量数量: {avg_num_svs_task1:.2f}\")\n",
    "    print(f\"  平均正确分类的离群点数量: {avg_correct_outliers_task1:.2f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 任务2: 不同C值下的分析 (gamma=1) --- [cite: 9, 10]\n",
    "    print(\"\\n--- 任务2: 不同C值下的分析 (gamma=1) ---\")\n",
    "    gamma_val_task2 = 1.0\n",
    "    C_values_task2 = [0.01, 0.1, 1, 10, 50, 100] # 至少5个C值\n",
    "    results_table_task2 = []\n",
    "\n",
    "    for C_val in C_values_task2:\n",
    "        current_C_num_svs = []\n",
    "        current_C_correct_outliers = []\n",
    "        current_C_incorrect_outliers = []\n",
    "\n",
    "        for sim_idx in range(n_simulations):\n",
    "            test_indices = np.arange(sim_idx, n_total_samples, n_simulations)\n",
    "            train_indices = np.array([i for i in range(n_total_samples) if i not in test_indices])\n",
    "            X_train, y_train = X[train_indices], y[train_indices]\n",
    "            # X_test, y_test = X[test_indices], y[test_indices] # 测试集在此部分不直接用于表格中的指标\n",
    "\n",
    "            alpha_sv, X_sv_runtime, y_sv_runtime, b_runtime, all_train_alphas = train_svm(X_train, y_train, C_val, gamma_val_task2, tol=1e-5)\n",
    "\n",
    "            if len(alpha_sv) == 0:\n",
    "                print(f\"  C={C_val}, 模拟 {sim_idx+1}: 未找到支持向量，跳过。\")\n",
    "                current_C_num_svs.append(0)\n",
    "                current_C_correct_outliers.append(0)\n",
    "                current_C_incorrect_outliers.append(0)\n",
    "                continue\n",
    "\n",
    "            current_C_num_svs.append(len(alpha_sv))\n",
    "\n",
    "            # 离群点分析 (alpha_i approx C)\n",
    "            outlier_candidate_indices = np.where(np.abs(all_train_alphas - C_val) < epsilon_alpha_is_C)[0]\n",
    "            count_correct_outliers_curr_c = 0\n",
    "            count_incorrect_outliers_curr_c = 0\n",
    "\n",
    "            if len(outlier_candidate_indices) > 0:\n",
    "                # 需要对这些离群点候选者进行分类判断\n",
    "                # 我们需要对训练集进行预测来判断这些 alpha=C 的点是否被正确分类\n",
    "                train_preds_for_outliers, _ = predict_svm(X_train, alpha_sv, X_sv_runtime, y_sv_runtime, b_runtime, gamma_val_task2)\n",
    "\n",
    "                for idx_in_train in outlier_candidate_indices:\n",
    "                    if train_preds_for_outliers[idx_in_train] == y_train[idx_in_train]:\n",
    "                        count_correct_outliers_curr_c += 1\n",
    "                    else:\n",
    "                        count_incorrect_outliers_curr_c += 1\n",
    "            current_C_correct_outliers.append(count_correct_outliers_curr_c)\n",
    "            current_C_incorrect_outliers.append(count_incorrect_outliers_curr_c)\n",
    "            # print(f\"  C={C_val}, 模拟 {sim_idx+1}: SVs={len(alpha_sv)}, CorrectOutliers={count_correct_outliers_curr_c}, IncorrectOutliers={count_incorrect_outliers_curr_c}\")\n",
    "\n",
    "\n",
    "        avg_num_svs_curr_c = np.mean(current_C_num_svs)\n",
    "        avg_correct_outliers_curr_c = np.mean(current_C_correct_outliers)\n",
    "        avg_incorrect_outliers_curr_c = np.mean(current_C_incorrect_outliers)\n",
    "        results_table_task2.append({\n",
    "            \"C\": C_val,\n",
    "            \"Average number of support vectors\": f\"{avg_num_svs_curr_c:.2f}\",\n",
    "            \"Average number of correct outliers\": f\"{avg_correct_outliers_curr_c:.2f}\",\n",
    "            \"Average number of incorrect outliers\": f\"{avg_incorrect_outliers_curr_c:.2f}\"\n",
    "        })\n",
    "        print(f\"  C={C_val}: Avg SVs={avg_num_svs_curr_c:.2f}, Avg Correct Outliers={avg_correct_outliers_curr_c:.2f}, Avg Incorrect Outliers={avg_incorrect_outliers_curr_c:.2f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n任务2 表格结果[cite: 10]:\")\n",
    "    df_task2 = pd.DataFrame(results_table_task2)\n",
    "    print(df_task2.to_string())\n",
    "\n",
    "    print(\"\\n任务2 趋势分析:\")\n",
    "    print(\"1. C 与支持向量数量的关系:\")\n",
    "    print(\"   - 当 C 值较小时，模型对误分类的惩罚较小，倾向于选择一个更宽的间隔（margin），即使这意味着一些点会落入间隔内或被错误分类。这通常导致较少的支持向量，主要是那些定义间隔边界的点。\")\n",
    "    print(\"   - 当 C 值增大时，模型对误分类的惩罚变大。模型会试图将尽可能多的点正确分类，即使这意味着间隔会变窄。这会导致更多的点成为支持向量，因为模型对数据拟合得更紧密。\")\n",
    "    print(\"   - 因此，通常情况下，随着 C 的增加，支持向量的数量会增加或保持在一个较高的水平，因为更多的点会影响决策边界的确定。\")\n",
    "    print(\"\\n2. C 与正确/错误分类的离群点数量的关系 (离群点指 alpha_i = C 的点):\")\n",
    "    print(\"   - 正确分类的离群点 (alpha_i=C, y_i * f(x_i) > 0): 这些是位于间隔边界上或间隔内但被正确分类的点。\")\n",
    "    print(\"   - 错误分类的离群点 (alpha_i=C, y_i * f(x_i) < 0): 这些是被错误分类的点。\")\n",
    "    print(\"   - 当 C 值较小时，模型容忍更多的错误和间隔内的点。可能有较多的点其 alpha 达到 C，其中一些可能是被容忍的错误分类（错误离群点），另一些可能是间隔内被正确分类的点（正确离群点）。\")\n",
    "    print(\"   - 当 C 值增大时，模型对错误的容忍度降低。\")\n",
    "    print(\"     - 错误分类的离群点数量可能会减少，因为模型努力避免它们。然而，如果数据非常复杂或有噪声，即使C很大，也可能存在一些顽固的错误分类点。\")\n",
    "    print(\"     - 正确分类的离群点（那些在间隔内但被正确分类的点，alpha=C）的数量可能会变化。如果C很大，模型试图使间隔“干净”，可能会减少这类点。但同时，更多的点可能成为边界上的支持向量（alpha < C）。对于alpha=C的点，它们是那些模型“尽力”也无法使其满足 y_i*f(x_i) >= 1 的点。\")\n",
    "    print(\"   - 总的来说，随着 C 增加：\")\n",
    "    print(\"     - 模型对训练数据的拟合程度增加。\")\n",
    "    print(\"     - 错误分类的离群点数量（alpha=C且错分）一般期望减少，因为惩罚变大了。\")\n",
    "    print(\"     - 正确分类的离群点数量（alpha=C且正分，但 $y_i f(x_i) \\le 1$）的行为可能更复杂，取决于数据集。如果C非常大，模型会尝试将所有点都正确分类并推出间隔，所以理论上这类点（alpha=C）应该主要是那些难以处理的点。\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "    # --- 任务3: 超参数调优 --- [cite: 11]\n",
    "    print(\"\\n--- 任务3: 超参数调优 ---\")\n",
    "    # 设定gamma和C的候选值范围\n",
    "    gamma_values_tune = [0.01, 0.1, 0.5, 1, 5, 10]\n",
    "    C_values_tune = [0.1, 1, 10, 50, 100, 200]\n",
    "\n",
    "    best_avg_test_error = float('inf')\n",
    "    best_gamma = None\n",
    "    best_C = None\n",
    "    all_tuning_results = []\n",
    "\n",
    "    print(f\"开始超参数调优，Gamma候选: {gamma_values_tune}, C候选: {C_values_tune}\")\n",
    "    total_configs = len(gamma_values_tune) * len(C_values_tune)\n",
    "    config_count = 0\n",
    "\n",
    "    for gamma_val in gamma_values_tune:\n",
    "        for C_val in C_values_tune:\n",
    "            config_count += 1\n",
    "            # print(f\"  调优进度: {config_count}/{total_configs} (Gamma={gamma_val}, C={C_val})\")\n",
    "            current_config_test_errors = []\n",
    "            for sim_idx in range(n_simulations):\n",
    "                test_indices = np.arange(sim_idx, n_total_samples, n_simulations)\n",
    "                train_indices = np.array([i for i in range(n_total_samples) if i not in test_indices])\n",
    "                X_train, y_train = X[train_indices], y[train_indices]\n",
    "                X_test, y_test = X[test_indices], y[test_indices]\n",
    "\n",
    "                alpha_sv, X_sv_runtime, y_sv_runtime, b_runtime, _ = train_svm(X_train, y_train, C_val, gamma_val, tol=1e-5)\n",
    "\n",
    "                if len(alpha_sv) == 0:\n",
    "                    current_config_test_errors.append(0.5) # 假设错误率\n",
    "                    continue\n",
    "\n",
    "                test_preds, _ = predict_svm(X_test, alpha_sv, X_sv_runtime, y_sv_runtime, b_runtime, gamma_val)\n",
    "                test_error = np.mean(test_preds != y_test)\n",
    "                current_config_test_errors.append(test_error)\n",
    "\n",
    "            avg_test_error_curr_config = np.mean(current_config_test_errors)\n",
    "            all_tuning_results.append({'gamma': gamma_val, 'C': C_val, 'avg_test_error': avg_test_error_curr_config})\n",
    "            print(f\"  Gamma={gamma_val}, C={C_val}: 平均测试误差 = {avg_test_error_curr_config:.4f}\")\n",
    "\n",
    "            if avg_test_error_curr_config < best_avg_test_error:\n",
    "                best_avg_test_error = avg_test_error_curr_config\n",
    "                best_gamma = gamma_val\n",
    "                best_C = C_val\n",
    "\n",
    "    print(\"\\n任务3 超参数调优结果:\")\n",
    "    # 可以选择打印所有调优结果\n",
    "    # df_tuning = pd.DataFrame(all_tuning_results)\n",
    "    # print(df_tuning.sort_values(by='avg_test_error').to_string())\n",
    "\n",
    "    if best_gamma is not None and best_C is not None:\n",
    "        print(f\"  最佳平均测试误差: {best_avg_test_error:.4f}\")\n",
    "        print(f\"  对应的最佳 Gamma: {best_gamma}\")\n",
    "        print(f\"  对应的最佳 C: {best_C}\")\n",
    "    else:\n",
    "        print(\"  未能找到有效的超参数组合（可能所有组合都没有支持向量）。\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"\\n代码执行完毕。\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
