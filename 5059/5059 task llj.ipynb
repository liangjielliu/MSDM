{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fff638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "任务1: C=10, gamma=1\n",
      "平均训练误差: 0.0000, 标准差: 0.0000\n",
      "平均测试误差: 0.1202, 标准差: 0.0754\n",
      "平均支持向量数量: 142.85\n",
      "平均正确离群点数量: 0.00\n",
      "\n",
      "任务2: 分析不同 C 值对离群点和支持向量的影响\n",
      "        C     Avg SVs  Correct Outliers  Incorrect Outliers\n",
      "0    0.01  184.076923         84.307692           89.538462\n",
      "1    0.10  183.384615         89.923077           84.307692\n",
      "2    1.00  153.769231         62.538462            1.769231\n",
      "3   10.00  142.846154          0.000000            0.000000\n",
      "4   50.00  142.846154          0.000000            0.000000\n",
      "5  100.00  142.846154          0.000000            0.000000\n",
      "\n",
      "任务3: 超参数调优\n",
      "Gamma=0.01, C=0.1 => 平均测试误差: 0.4663\n",
      "Gamma=0.01, C=1 => 平均测试误差: 0.4375\n",
      "Gamma=0.01, C=10 => 平均测试误差: 0.2212\n",
      "Gamma=0.01, C=50 => 平均测试误差: 0.1827\n",
      "Gamma=0.01, C=100 => 平均测试误差: 0.1827\n",
      "Gamma=0.1, C=0.1 => 平均测试误差: 0.4663\n",
      "Gamma=0.1, C=1 => 平均测试误差: 0.2019\n",
      "Gamma=0.1, C=10 => 平均测试误差: 0.1394\n",
      "Gamma=0.1, C=50 => 平均测试误差: 0.1250\n",
      "Gamma=0.1, C=100 => 平均测试误差: 0.1346\n",
      "Gamma=0.5, C=0.1 => 平均测试误差: 0.3750\n",
      "Gamma=0.5, C=1 => 平均测试误差: 0.1442\n",
      "Gamma=0.5, C=10 => 平均测试误差: 0.0962\n",
      "Gamma=0.5, C=50 => 平均测试误差: 0.0962\n",
      "Gamma=0.5, C=100 => 平均测试误差: 0.0962\n",
      "Gamma=1, C=0.1 => 平均测试误差: 0.4519\n",
      "Gamma=1, C=1 => 平均测试误差: 0.1346\n",
      "Gamma=1, C=10 => 平均测试误差: 0.1202\n",
      "Gamma=1, C=50 => 平均测试误差: 0.1202\n",
      "Gamma=1, C=100 => 平均测试误差: 0.1202\n",
      "Gamma=5, C=0.1 => 平均测试误差: 0.4663\n",
      "Gamma=5, C=1 => 平均测试误差: 0.1731\n",
      "Gamma=5, C=10 => 平均测试误差: 0.1683\n",
      "Gamma=5, C=50 => 平均测试误差: 0.1683\n",
      "Gamma=5, C=100 => 平均测试误差: 0.1683\n",
      "最优结果: Gamma=0.5, C=10, 测试误差=0.0962\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "\n",
    "# --------------- 数据加载与预处理 ---------------\n",
    "def load_sonar_data(file_path):\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y_labels = data.iloc[:, -1].values\n",
    "    y = np.array([1 if label == 'M' else -1 for label in y_labels])\n",
    "    return X, y\n",
    "\n",
    "# --------------- 核函数 ---------------\n",
    "def rbf_kernel(x1, x2, gamma):\n",
    "    return np.exp(-gamma * np.dot(x1 - x2, x1 - x2))\n",
    "\n",
    "def compute_kernel_matrix(X, gamma):\n",
    "    sq_dists = np.sum(X**2, axis=1)[:, None] + np.sum(X**2, axis=1)[None, :] - 2 * np.dot(X, X.T)\n",
    "    return np.exp(-gamma * sq_dists)\n",
    "\n",
    "# --------------- SVM 训练函数 ---------------\n",
    "def train_svm(X_train, y_train, C, gamma, tol=1e-5):\n",
    "    n = len(X_train)\n",
    "    K = compute_kernel_matrix(X_train, gamma)\n",
    "    Q = np.outer(y_train, y_train) * K\n",
    "\n",
    "    def objective(alpha):\n",
    "        return 0.5 * alpha @ Q @ alpha - np.sum(alpha)\n",
    "\n",
    "    def gradient(alpha):\n",
    "        return Q @ alpha - np.ones(n)\n",
    "\n",
    "    constraints = {'type': 'eq', 'fun': lambda a: np.dot(a, y_train), 'jac': lambda a: y_train}\n",
    "    bounds = [(0, C)] * n\n",
    "    alpha0 = np.zeros(n)\n",
    "\n",
    "    res = minimize(objective, alpha0, jac=gradient, bounds=bounds, constraints=constraints,\n",
    "                   method='SLSQP', tol=1e-6, options={'disp': False, 'maxiter': 1000})\n",
    "\n",
    "    if not res.success:\n",
    "        print(\"SLSQP 优化失败:\", res.message)\n",
    "        return np.array([]), np.array([]), np.array([]), 0.0, np.zeros(n)\n",
    "\n",
    "    alphas = res.x\n",
    "    sv_mask = alphas > tol\n",
    "    alpha_sv = alphas[sv_mask]\n",
    "    X_sv = X_train[sv_mask]\n",
    "    y_sv = y_train[sv_mask]\n",
    "\n",
    "    if len(alpha_sv) == 0:\n",
    "        return alpha_sv, X_sv, y_sv, 0.0, alphas\n",
    "\n",
    "    margin_mask = (alphas > tol) & (alphas < C - tol)\n",
    "    b_list = []\n",
    "    for i in np.where(margin_mask)[0]:\n",
    "        s = sum(alpha_sv[j] * y_sv[j] * rbf_kernel(X_sv[j], X_train[i], gamma) for j in range(len(alpha_sv)))\n",
    "        b_list.append(y_train[i] - s)\n",
    "\n",
    "    b = np.mean(b_list) if b_list else 0.0\n",
    "    return alpha_sv, X_sv, y_sv, b, alphas\n",
    "\n",
    "# --------------- SVM 预测函数 ---------------\n",
    "def predict_svm(X_eval, alpha_sv, X_sv, y_sv, b, gamma):\n",
    "    if len(alpha_sv) == 0:\n",
    "        scores = np.full(X_eval.shape[0], b)\n",
    "        preds = np.sign(scores)\n",
    "        preds[preds == 0] = 1\n",
    "        return preds, scores\n",
    "\n",
    "    scores = np.array([\n",
    "        sum(alpha_sv[j] * y_sv[j] * rbf_kernel(X_sv[j], x, gamma) for j in range(len(alpha_sv))) + b\n",
    "        for x in X_eval\n",
    "    ])\n",
    "    preds = np.sign(scores)\n",
    "    preds[preds == 0] = 1\n",
    "    return preds, scores\n",
    "\n",
    "# --------------- 主程序（任务1~3） ---------------\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = load_sonar_data(\"sonar.all-data\")\n",
    "    n = len(X)\n",
    "    n_splits = 13\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    # ---------- 任务1 ----------\n",
    "    print(\"任务1: C=10, gamma=1\")\n",
    "    C, gamma = 10, 1\n",
    "    train_errors, test_errors, sv_counts, correct_outliers = [], [], [], []\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        test_idx = np.arange(i, n, n_splits)\n",
    "        train_idx = np.setdiff1d(np.arange(n), test_idx)\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "        alpha_sv, X_sv, y_sv, b, alphas = train_svm(X_train, y_train, C, gamma)\n",
    "        if len(alpha_sv) == 0:\n",
    "            train_errors.append(0.5)\n",
    "            test_errors.append(0.5)\n",
    "            sv_counts.append(0)\n",
    "            correct_outliers.append(0)\n",
    "            continue\n",
    "\n",
    "        train_preds, _ = predict_svm(X_train, alpha_sv, X_sv, y_sv, b, gamma)\n",
    "        test_preds, _ = predict_svm(X_test, alpha_sv, X_sv, y_sv, b, gamma)\n",
    "        train_errors.append(np.mean(train_preds != y_train))\n",
    "        test_errors.append(np.mean(test_preds != y_test))\n",
    "        sv_counts.append(len(alpha_sv))\n",
    "\n",
    "        outlier_mask = np.isclose(alphas, C, atol=epsilon)\n",
    "        correct = sum(train_preds[i] == y_train[i] for i in range(len(y_train)) if outlier_mask[i])\n",
    "        correct_outliers.append(correct)\n",
    "\n",
    "    print(f\"平均训练误差: {np.mean(train_errors):.4f}, 标准差: {np.std(train_errors):.4f}\")\n",
    "    print(f\"平均测试误差: {np.mean(test_errors):.4f}, 标准差: {np.std(test_errors):.4f}\")\n",
    "    print(f\"平均支持向量数量: {np.mean(sv_counts):.2f}\")\n",
    "    print(f\"平均正确离群点数量: {np.mean(correct_outliers):.2f}\")\n",
    "\n",
    "    # ---------- 任务2 ----------\n",
    "    print(\"\\n任务2: 分析不同 C 值对离群点和支持向量的影响\")\n",
    "    C_values = [0.01, 0.1, 1, 10, 50, 100]\n",
    "    result_table = []\n",
    "\n",
    "    for C in C_values:\n",
    "        svs, correct_outs, incorrect_outs = [], [], []\n",
    "        for i in range(n_splits):\n",
    "            test_idx = np.arange(i, n, n_splits)\n",
    "            train_idx = np.setdiff1d(np.arange(n), test_idx)\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "\n",
    "            alpha_sv, X_sv, y_sv, b, alphas = train_svm(X_train, y_train, C, gamma)\n",
    "            if len(alpha_sv) == 0:\n",
    "                svs.append(0)\n",
    "                correct_outs.append(0)\n",
    "                incorrect_outs.append(0)\n",
    "                continue\n",
    "\n",
    "            train_preds, _ = predict_svm(X_train, alpha_sv, X_sv, y_sv, b, gamma)\n",
    "            outlier_mask = np.isclose(alphas, C, atol=epsilon)\n",
    "            correct = sum(train_preds[i] == y_train[i] for i in range(len(y_train)) if outlier_mask[i])\n",
    "            incorrect = sum(train_preds[i] != y_train[i] for i in range(len(y_train)) if outlier_mask[i])\n",
    "\n",
    "            svs.append(len(alpha_sv))\n",
    "            correct_outs.append(correct)\n",
    "            incorrect_outs.append(incorrect)\n",
    "\n",
    "        result_table.append({\n",
    "            \"C\": C,\n",
    "            \"Avg SVs\": np.mean(svs),\n",
    "            \"Correct Outliers\": np.mean(correct_outs),\n",
    "            \"Incorrect Outliers\": np.mean(incorrect_outs)\n",
    "        })\n",
    "\n",
    "    df2 = pd.DataFrame(result_table)\n",
    "    print(df2)\n",
    "\n",
    "    # ---------- 任务3 ----------\n",
    "    print(\"\\n任务3: 超参数调优\")\n",
    "    gamma_list = [0.01, 0.1, 0.5, 1, 5]\n",
    "    C_list = [0.1, 1, 10, 50, 100]\n",
    "    best_result = {\"gamma\": None, \"C\": None, \"error\": float('inf')}\n",
    "\n",
    "    for gamma in gamma_list:\n",
    "        for C in C_list:\n",
    "            test_errors = []\n",
    "            for i in range(n_splits):\n",
    "                test_idx = np.arange(i, n, n_splits)\n",
    "                train_idx = np.setdiff1d(np.arange(n), test_idx)\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "                alpha_sv, X_sv, y_sv, b, _ = train_svm(X_train, y_train, C, gamma)\n",
    "                if len(alpha_sv) == 0:\n",
    "                    test_errors.append(0.5)\n",
    "                    continue\n",
    "                preds, _ = predict_svm(X_test, alpha_sv, X_sv, y_sv, b, gamma)\n",
    "                test_errors.append(np.mean(preds != y_test))\n",
    "\n",
    "            avg_error = np.mean(test_errors)\n",
    "            if avg_error < best_result[\"error\"]:\n",
    "                best_result = {\"gamma\": gamma, \"C\": C, \"error\": avg_error}\n",
    "\n",
    "            print(f\"Gamma={gamma}, C={C} => 平均测试误差: {avg_error:.4f}\")\n",
    "\n",
    "    print(f\"最优结果: Gamma={best_result['gamma']}, C={best_result['C']}, 测试误差={best_result['error']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af3ea3",
   "metadata": {},
   "source": [
    "# Task 4 Report: Support Vector Classification on Sonar Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "This report presents the implementation and evaluation of a non-linear support vector machine (SVM) classifier applied to the UCI Sonar dataset. The classifier was implemented from scratch using the SLSQP optimizer with an RBF kernel. Three main tasks were performed:\n",
    "\n",
    "1. Evaluation of the classifier with fixed parameters `C = 10` and `gamma = 1`\n",
    "2. Analysis of how varying the penalty parameter `C` affects model characteristics\n",
    "3. Hyperparameter tuning of `C` and `gamma` to minimize the test error\n",
    "\n",
    "---\n",
    "\n",
    "## Task 1: Fixed Parameters (C=10, gamma=1)\n",
    "\n",
    "I evaluated the classifier using 13-fold stratified cross-validation, where each fold uses every 13th instance as the test set. The results were:\n",
    "\n",
    "- **Average training error**: **0.0000**\n",
    "- **Standard deviation (training)**: **0.0000**\n",
    "- **Average test error**: **0.1202**\n",
    "- **Standard deviation (test)**: **0.0754**\n",
    "- **Average number of support vectors**: **142.85**\n",
    "- **Average number of correctly classified outliers**: **0.00**\n",
    "\n",
    "These results indicate perfect training performance with a reasonable test error, but no support vectors were identified as outliers (i.e., dual variables α ≈ C).\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2: Effect of C on Support Vectors and Outliers\n",
    "\n",
    "I varied the value of `C` across a wide range and measured the effect on the number of support vectors and correctly/incorrectly classified outliers. The results are summarized below:\n",
    "\n",
    "| C       | Avg SVs  | Correct Outliers | Incorrect Outliers |\n",
    "|---------|----------|------------------|--------------------|\n",
    "| 0.01    | 184.08   | 84.31            | 89.54              |\n",
    "| 0.10    | 183.38   | 89.92            | 84.31              |\n",
    "| 1.00    | 153.77   | 62.54            | 1.77               |\n",
    "| 10.00   | 142.85   | 0.00             | 0.00               |\n",
    "| 50.00   | 142.85   | 0.00             | 0.00               |\n",
    "| 100.00  | 142.85   | 0.00             | 0.00               |\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- As **C increases**, the **number of support vectors decreases**, indicating a tighter decision boundary.\n",
    "- **Correct and incorrect outliers** are both high for small C, but drop to **zero** for `C ≥ 10`.\n",
    "- Small `C` leads to underfitting, assigning many training points as support vectors or outliers.\n",
    "- High `C` encourages fewer support vectors and no slack variables, reducing outliers entirely.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 3: Hyperparameter Tuning\n",
    "\n",
    "A grid search was performed over:\n",
    "\n",
    "- `gamma`: [0.01, 0.1, 0.5, 1, 5]\n",
    "- `C`: [0.1, 1, 10, 50, 100]\n",
    "\n",
    "Each combination was evaluated using 13-fold cross-validation, and the average test error was recorded.\n",
    "\n",
    "### Best Result:\n",
    "\n",
    "- **Optimal gamma**: **0.5**\n",
    "- **Optimal C**: **10**\n",
    "- **Minimum test error**: **0.0962**\n",
    "\n",
    "### Selected Results:\n",
    "\n",
    "| Gamma | C   | Avg Test Error |\n",
    "|-------|-----|----------------|\n",
    "| 0.5   | 10  | **0.0962**     |\n",
    "| 0.5   | 50  | 0.0962         |\n",
    "| 0.5   | 100 | 0.0962         |\n",
    "| 0.1   | 10  | 0.1394         |\n",
    "| 1.0   | 10  | 0.1202         |\n",
    "| 5.0   | 10  | 0.1683         |\n",
    "| 0.01  | 100 | 0.1827         |\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- Moderate values of `gamma` and `C` led to the lowest test error.\n",
    "- Extremely small `gamma` (e.g., 0.01) underfit the data.\n",
    "- Extremely large `gamma` (e.g., 5) increased overfitting and degraded performance.\n",
    "- Test error plateaued across some values of C, suggesting robustness in the optimal region.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The custom SVM implementation using the SLSQP optimizer effectively classified the Sonar dataset with competitive test accuracy. The tuning of hyperparameters revealed that moderate regularization (`C = 10`) and kernel flexibility (`gamma = 0.5`) achieve the best performance. Future improvements could involve incorporating visualization, confidence scores, and more advanced kernels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
